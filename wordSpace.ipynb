{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# WordSpace\n",
    "\n",
    "Tratasse de uma abordagem de vetorização de palavras com o intuito de preservar características semânticas e contextuais dos termos, além da construção de uma representação com redução de dimensionalidade, preservando as características anteriormente citadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos:\n",
    "- Preservar características **semânticas** dos termos baseado no uso de _thesaurus_ para enriquecimento semântico. \n",
    "- Preservar características **contextuais** dos termos baseado na exploração de _corpus_ envolvidos na representação. \n",
    "- **Redução de dimensionalidade** com relação à representação tradicional de palavras preservando os relacionamentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propriedades:\n",
    "### 1. Correlação semântica (positiva, neutra e negativa)\n",
    "A propriedade de correlação / similaridade está diretamente vinculada ao conceito de semântica de termos, isso é, palavras que possuem alto grau de similaridade, são sinônimas ou que possuem contextos muito próximos, tentem a ter alta correlação positiva. Já palavras que não são comparáveis, sendo termos independentes um com relação ao outro ou que não coocorrem nos mesmos contextos tendem a ter correlação neutra. Já termos que não são similares, sendo opostos entre si, como antônimos ou estão em contextos opostos, tende a ter grande correlação negativa.\n",
    "\n",
    "Exemplo:\n",
    "\n",
    "Considere as seguintes palavras: $quente$, $aquecido$, $dois$ e $frio$ e uma função $VEC(W)$ que recebe como entrada uma palavra e retorna o vetor que a representa. Considere também uma função $CORR(X, Y)$, que possui como entrada dois vetores e de saída fornece a correlação / similaridade entre eles da seguinte forma:\n",
    "> - $CORR(X, Y) \\approx 1 \\longleftrightarrow X$ e $Y$ possui correlação positiva.\n",
    "> - $CORR(X, Y) \\approx 0 \\longleftrightarrow X$ e $Y$ possui correlação neutra.\n",
    "> - $CORR(X, Y) \\approx -1 \\longleftrightarrow X$ e $Y$ possui correlação negativa.\n",
    "\n",
    "sendo assim, esperasse que:\n",
    "> - $CORR(VEC(quente), VEC(aquecido)) \\approx 1$, indicando correlação positiva.\n",
    "> - $CORR(VEC(quente), VEC(dois)) \\approx 0$, indicando correlação neutra.\n",
    "> - $CORR(VEC(quente), VEC(frio)) \\approx -1$, indicando correlação negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Álgebra vetorial\n",
    "Operações vetoriais são aplicáveis nesta nova representação e possuem significado semântico associado.\n",
    "\n",
    "#### Soma vetorial\n",
    "\n",
    "Dado dois vetores $X$ e $Y$ que representam duas palavras, o vetor $Z = X + Y$ representa uma nova palavra, que apresenta as características médias comuns a $X$ e $Y$.\n",
    "\n",
    "#### Multiplicação por escalar\n",
    "Dado um número real $k$ e o vetor $X$ que representa uma palavra, o vetor $Y = kX$ representa uma nova palavra, que apresenta a ponderação das características vinculadas ao vetor $X$.\n",
    "\n",
    "#### Observações\n",
    "Por preservar as características da álgebra vetorial, assim como suas operações, tais como a soma vetorial e multiplicação por escalar (além de outras como o produto interno, externo e norma), as propriedades vinculadas a essas operações também são preservadas, são elas: comutativa, associativa, associativa multiplicativa, distributiva por número real, distributiva por vetor, elemento neutro da adição, elemento neutro da multiplicação e elemento oposto. Algumas delas, por carregar elevado significado semântico / contextual serão discutidas a seguir:\n",
    "> **Propriedade do elemento oposto**\n",
    ">\n",
    "> Dado um vetor $X$ que representa uma palavra, o vetor $-X$ representa a palavra oposta a palavra representada pelo vetor $X$. \n",
    ">\n",
    "> Tomando o exemplo anteriormente citado, isso implica que a $CORR(X, -X) = -1$. Ou ainda, considerando as palavras $amor$ e $ódio$, assim como sua representação vetorial, tem-se que: $VEC(amor) \\approx -VEC(ódio)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Procedimentos para execução do método\n",
    "Abaixo segue a sequência de procedimentos para execução do método. Cada passo da execução acompanha sua respectiva explicação e tecnologias envolvidas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Procedimentos iniciais\n",
    "Carregamento de arquivo de _corpus_ textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No ,  he says now .\n",
      "And what did he do ?\n",
      " The money 's there .\n",
      "That was less than a year ago .\n",
      "But he made only the first .\n",
      "There 's still time for them to do it .\n",
      "But he should nt have .\n",
      " They have to come down to the people .\n",
      "I do nt know where that is...\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw.txt') as fl:\n",
    "    text = fl.read()\n",
    "\n",
    "print(text[:255] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Processamento de linguagem natural \n",
    "Foram aplicados procedimento de PLN no texto carregado, a fim de preparar o dado para retirar os relacionamentos e informações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Normalização\n",
    "Na etapa de normalização, transformou-se todos as letras para seu respectivo padrão em caixa-baixa e todo caractere de pontuação válido da língua inglesa foi separado de qualquer palavra vizinha a ele que possa surgir antes ou depois do sinal de pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no  ,   he says now  . \n",
      "and what did he do  ? \n",
      " the money 's there  . \n",
      "that was less than a year ago  . \n",
      "but he made only the first  . \n",
      "there 's still time for them to do it  . \n",
      "but he should nt have  . \n",
      " they have to come down to the people  . \n",
      "i do nt ...\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "for symbol in '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~':\n",
    "    text = text.replace(symbol, ' ' + symbol + ' ')\n",
    "\n",
    "print(text[:255] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tokenização\n",
    "Na etapa de tokenização, o texto do _corpus_ carregado foi fragmentado em palavras separados por caractere brancos tais como: quebra de linha, espaço, tabulação...\n",
    "\n",
    "É relavante resaltar que ao realizar a tokenização do texto uma sequência é gerada. Uma sequência é uma lista que preserva a ocorrência e possição dos termos presentes no _corpus_ textual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no', ',', 'he', 'says', 'now', '.', 'and', 'what', 'did', 'he', 'do', '?', ...\n"
     ]
    }
   ],
   "source": [
    "sequence = text.split()\n",
    "print(str(sequence[:12])[:-1] + \", ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mapeamento de palavras\n",
    "Para criação de uma representação de palavras, precisasse de uma mapeamento dos termos do corpus, de forma a aplicar para cada termos apenas um número capaz de indexar o vetor associado ao termo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'of', 'days', 'business', 'know', 'work', 'our', 'dr', 'former', 'only', 'back', 'best', ...\n",
      "\n",
      "{',': 0, 'of': 1, 'days': 2, 'business': 3, 'know': 4, 'work': 5, 'our': 6, 'dr': 7, 'former': 8, 'o...\n"
     ]
    }
   ],
   "source": [
    "indexes = list(set(sequence))\n",
    "words = {k: i for i, k in enumerate(indexes)}\n",
    "print(str(indexes[:12])[:-1] + \", ...\\n\")\n",
    "print(str(words)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Construção de matriz de representação de palavras no formato padrão\n",
    "Desejasse produzir uma matriz esparsa que demonstre as características de relacionamento esperado para as palavras. Sabe-se que a representação padrão dos termos na sua forma vetorial não é cômoda para o processamento devido duas principais características:\n",
    "\n",
    "- Os vetores são demasiadamente longos, sendo custoso para processamento computacional.\n",
    "- Os vetores são esparsos, muitas das palavras adicionadas não possuem relacionamento, tendendo ao _overfitting_ no treinamento de modelos.\n",
    "\n",
    "Devido a alta densidade de dados na matriz, por se tratar de uma $N\\times N$, onde $N$ é o tamanho do vocabulário de termos, utiliza-se da propriedade de esparsividade da matriz para realizar uma representação computacional que seja menos custosa em termos de memória.\n",
    "\n",
    "Vale ressaltar que essa matriz conterá valores apenas no intervalo $[-1, 1]$. O objetivo é que cada célula represente o relacionamento direto do par correspondente, sendo $1$ o máximo relacionamento positivo e $-1$ o máximo relacionamento negativo. Valores próximos a zero define a independência de relacionamento do par."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "size = len(words)\n",
    "from scipy import sparse as sp\n",
    "matrix = sp.lil_matrix((size, size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aplicação de informação de contexto\n",
    "É desejável que as informações de palavras representadas vetorialmente carreguem informação de contexto, de forma a gerar um enriquecimento inicial da matriz de representação. Isso faz com que a matriz perca boa parte das características de independência linear, produzindo relacionamentos entre os vetores, viabilizando procedimentos, como a redução de dimensionalidade, aumentando a eficiência do método.\n",
    "\n",
    "Esse procedimento será realizado através da _**Informação Mútua**_ associada aos termos do _corpus_. A informação mútua é definida pela seguinte equação:\n",
    "\n",
    "\\begin{equation}\n",
    "I(x, y) = P(x, y) * log\\bigg(\\frac{P(x, y)}{P(x) * P(y)}\\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "<center><br>\n",
    "onde $P(j)$ é a probabilidade de ocorrência da palavra $j$ <br>\n",
    "e $P(j, k)$ é a probabilidade de ocorrência das palavras $j$ e $k$\n",
    "</center>\n",
    "\n",
    "O valor de $I(x, y)$ está intrinsecamente ligado ao conceito de entrópia, sendo assim, o valor esperado para informação mútua é que $I(x, y) \\geq 0$. Contudo, como citado anteriormente, deseja-se que os valores da matriz de relacionamento de termos esteja no intervalo $[-1, 1]$, o que não ocorre na expressão acima. Para fins práticos, a obtenção de uma probabilidade que indicasse a mutualidade de informação seria muito mais conveniente, então modificou-se a equação de informação mútual para a seguir, com o objetivo de garantir essa propriedade:\n",
    "\n",
    "\\begin{equation}\n",
    "I(x, y) = \\frac{P(x, y)^2}{P(x) * P(y)}\n",
    "\\end{equation}\n",
    "\n",
    "Para esta expressão, os valores de $I(x, y)$ estam garantidamente entre o intervalo $[0, 1]$, pois é garantida a propriedade que $P(j, k) \\leq P(j)$ e $P(j, k) \\leq P(k)$, sendo assim $P(j, k)^2 \\leq P(j) * P(k)$ garantindo os valores no intervalo desejado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O conceito de janela\n",
    "Dado uma sequência de termos $S$, uma janela $W$ é uma subsequência própria e não nula de termos em $S$, isso é, $W ⪱ S$, e $|W| > 0$. \n",
    "\n",
    "\\begin{equation}\n",
    "S = (... , s_{i-4}, s_{i-3} ,[ w_{i-n}, ... , w_{i-1}, w_i, w_{i+1}, ... , w_{i+n} ], s_{i+3}, s_{i+4}, ...)\n",
    "\\end{equation}\n",
    "\n",
    "Na expressão acima, os colchetes determinam a região da janela $W$, o número natural $i$ determina a posição da janela com relação a seguência $S$, o termo $W_i$ é o termo alvo da janela, e todos os termos $w_x$ onde $x \\in \\mathbb{N}$ e $i-n \\leq x \\leq i+n$ também pertecem a janela.\n",
    "\n",
    "Para fins de aplicação do conceito de janela no contexto de sequência de termos é de interesse que as janelas tenham tamanho impar. Sendo assim, o tamanho da janela, ou $|W|$, é definido da seguinte maneira:\n",
    "\n",
    "\\begin{equation}\n",
    "|W| = 2 * n + 1\n",
    "\\end{equation}\n",
    "\n",
    "<center><br>\n",
    "onde $n$ é qualquer número natural que satisfaz a relação $|W| < |S|$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Informação mútua e janela\n",
    "Diferente das aplicações tradicionais de informação mútua em recuperação da informação que buscam definir a ocorrência mútua de termos com relação aos documentos do _corpus_, será utilizado o conceito de janela com o objetivo de definir a existência de relacionamento entre dois termos da seguinte forma:\n",
    "\n",
    "Dado uma janela $W$ na posição $i$ da seguência $S$, o termo $s$ com $s \\in S$, coocorrê com o termo $w_i$ se somente se $s \\in W$.\n",
    "\n",
    "Observe que a informação mútua com relação a uma janela é muito mais restritiva que a relacionada a documentos, pois exerce alto grau de proximidade entre os termos. Vale resaltar que dentro de uma janela, a posição relativa do termo não é relevante, diferente de outras definições de janela ainda mais restritivas, como no _Word2Vec_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Informação mútua por frequência\n",
    "Diferente da definição acima especifica, que apresenta uma equação de informação mutual com dados normalizados (probabilidade), será utilizado a frequência da coocorrência dos termos em uma janela para calcular a informação mutua associada ao par de termos, definindo a equação da seguinte maneira:\n",
    "\n",
    "\\begin{equation}\n",
    "I(x, y) = \\frac{FREQ(x, y)^2}{FREQ(x) * FREQ(y)}\n",
    "\\end{equation}\n",
    "\n",
    "<center><br>\n",
    "onde $FREQ(j)$ é a frequência da ocorrência da palavra $j$ <br>\n",
    "e $FREQ(j, k)$ é a frequência da ocorrência das palavras $j$ e $k$ na mesma janela.\n",
    "</center>\n",
    "\n",
    "Observe que os valores de $I(x, y)$ continuam garantidamente entre o intervalo $[0, 1]$, pois a propriedade de $FREQ(j, k) \\leq FREQ(j)$ e $FREQ(j, k) \\leq FREQ(k)$ permanecem válidas, sendo assim $FREQ(j, k)^2 \\leq FREQ(j) * FREQ(k)$ garante os valores no intervalo desejado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a frequencia dos termos e coocorrencia nas janelas\n",
    "windowSize = 2\n",
    "sequenceSize = len(sequence)\n",
    "for i in range(windowSize, sequenceSize - windowSize):\n",
    "    wi = words[sequence[i]]\n",
    "    for n in range(i - windowSize, i + windowSize + 1):\n",
    "        sn = words[sequence[n]]\n",
    "        matrix[wi,sn] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a informação mutua pela frequencia\n",
    "for i in range(matrix.shape[0]):\n",
    "    for j in range(matrix.shape[1]):\n",
    "        if i != j and matrix[i,j] != 0:\n",
    "            matrix[i,j] = (matrix[i,j]**2) / (matrix[i,i] * matrix[j,j])\n",
    "for i in range(min(matrix.shape[0], matrix.shape[1])):\n",
    "    matrix[i,i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriquecimento semântico por _thesaurus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redução de dimensionalidade\n",
    "Existem na literatura variadas técnicas voltadas a redução de dimensionalidade, desde as mais básicas como: _Missing Values Ratio_ ou _Low Variance Filter_. Até tecnicas mais avançadas como: _Random Forest_ ou _Autoencoder_. Contudo, uma das técnicas estatísticas mais utilizada e difundidas para redução de dimensão é o _Principal Component Analysis_ (_PCA_). A técnica de _PCA_ parte do principio algébrico da transformação linear, para levar os dados de um determinado espaço dimensional para outro espaço dimensional ortogonal com menor dimensão, de tal forma que a transformação carregue para esse novo espaço dimensional a maior parte da informação da variância dos dados. \n",
    "\n",
    "A técnica de PCA funciona muito bem para preservar as informação de variância associada aos dados, mas é carrente em termos de legibilidade e interpretação das informações após a transformação. Isso é um comportamente esperado, tendo em vista que a técnica de _PCA_ busca evidência uma determinada propriedade (variância) em detrimento de outras propriedades, como o distância euclidiana ou dimensões interpretáveis. Sendo assim, pode-se exergar o _PCA_ como um evidenciador de propriedade, não necessariamente de variância, mas de qualquer propriedade, a depender da matriz para geração da transformação na técnica.\n",
    "\n",
    "Originalmente, na técnica de _PCA_ utilizamos a variância / covariância como propriedade a ser preservada, por associarmos o grau de informação ao grau de dispersão (variação) dos dados. Contudo, há uma propriedade de especial interesse para essa nova abordagem de vetorização de palavras, que é a similaridade coseno. Como discutindo anteriormente, deseja-se uma medida no intervalo $[-1, 1]$, que determina a correlação entre duas palavras na sua forma vetorizada. Essa medida de correlação pode ser vista como o coseno entre dois vetores. Observe que a matriz do coseno não se distância tanto da matriz de covariância. \n",
    "\n",
    "A covariância é definida pela seguinte equação:\n",
    "\n",
    "\\begin{equation}\n",
    "COV(X, Y) = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x}) * (y_i - \\bar{y})}{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "Contudo, pode-se enxergar a correlação _pearson_ como uma medida de covariância normalizada para o intervalo $[-1, 1]$ atravéz do desvio padrão. A correlação _pearson_ é definida abaixo:\n",
    "\n",
    "\\begin{equation}\n",
    "PEARSON(X, Y) = \\frac{COV(X, Y)}{\\sqrt{VAR(X) * VAR(Y)}}\n",
    "\\end{equation}\n",
    "\n",
    "<center><br>\n",
    "onde $VAR(V)$ é a variância no vetor $V$.\n",
    "</center>\n",
    "\n",
    "Contudo, se analisar a correlação _pearson_ por uma interpretação geométrica, observa-se que se trata da similaridade coseno entre dois vetores, com a origem deslocada pela média dos vetores. Então, se considerar a média na origem do espaço dimensional, a correlação _pearson_ e a similaridade coseno são as mesmas medidas. Logo também pode-se enxergar o coseno como uma medida de correlação que também evidência a dispersão dos dados.\n",
    "\n",
    "O coseno é definido abaixo:\n",
    "\n",
    "\\begin{equation}\n",
    "COS(X, Y) = \\frac{<X, Y>}{||X|| * ||Y||}\n",
    "\\end{equation}\n",
    "\n",
    "<center><br>\n",
    "onde $<V, Z>$ é o produto interno convêncional dos vetores $V$ e $Z$<br>\n",
    "e $||V||$ é a norma do vetor $V$.\n",
    "</center>\n",
    "\n",
    "No escopo deste projeto, é possível a aplicação da matriz coseno como a matriz de geração de transformação do _PCA_, apenas pelo fato do espaço de instância (linhas da matriz) ser o mesmo espaço de atributos (colunas da matriz), isso é, em termos de representação de palavras, as linhas representam palavras que são descritas em colunas com essas mesmas palavras, permitinado o calculo do coseno termo a termo da matriz, produzindo uma matriz simétrica com a mesma dimensão do problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from math import sqrt, ceil\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "k = int(ceil(sqrt(matrix.shape[1]))) # estamos selecioando sqrt(N) dimensões, onde N é o tamanho do vocabulário.\n",
    "cosine = cosine_similarity(matrix, dense_output=False)\n",
    "w, v = sp.linalg.eigs(cosine, k=k, which='LM')\n",
    "order = np.argsort(w)[::-1]\n",
    "vectors = v[:,order].astype(np.float)\n",
    "values = w[order].astype(np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação de vetores\n",
    "Obtida a matriz de transformação, basta multiplicar a matriz de dados de palavras pela matrix gerada no _PCA_ de forma a obter a nova representação com dimensão reduzida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = matrix.dot(vectors[:,:k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mensurando a qualidade da representação\n",
    "Pode-se mensurar o resultado obtido com relação aos dados originais atravês de qualquer medida de análise de regressão, como o _mean absolute error_ (_MAE_), se comparar a matriz de coseno dos dados originais com a matriz coseno dos dados transformados, pode-se aplicar o calculo de _MAE_ para determinar a distância entre as duas representações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 89.14%\n"
     ]
    }
   ],
   "source": [
    "trans = sp.csr_matrix(data)\n",
    "delta = cosine - cosine_similarity(trans, dense_output=False)\n",
    "total = delta.shape[0] * delta.shape[1]\n",
    "error = abs(delta).sum() / (2 * total)\n",
    "print(\"Acurácia: \" + str(round((1 - error) * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo vetores similares\n",
    "Pode-se aplicar a similaridade coseno como uma medida para mensurar vetores próximos a um dado vetor, a fim de obter palavras similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra alvo: day\n",
      "Palavras relacionadas:\n",
      "    every: \t0.9985856639120949\n",
      "    by: \t0.9958301263639007\n",
      "    night: \t0.9941734986373423\n",
      "    off: \t0.9462417378034336\n",
      "    another: \t0.920935540348753\n",
      "    next: \t0.8687166868533766\n",
      "    ms: \t0.815664437062833\n",
      "    last: \t0.798347650328012\n",
      "    year: \t0.7146046402644263\n",
      "    after: \t0.675351358947759\n"
     ]
    }
   ],
   "source": [
    "word = 'day'\n",
    "wordIndex = words[word]\n",
    "similarWords = cosine_similarity([data[wordIndex]], data)[0]\n",
    "orderWords = np.argsort(similarWords)[::-1]\n",
    "print('Palavra alvo: ' + word)\n",
    "print('Palavras relacionadas:')\n",
    "for i in orderWords[1:11]:\n",
    "    print(\"    \" + indexes[i] + \": \\t\" + str(similarWords[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
